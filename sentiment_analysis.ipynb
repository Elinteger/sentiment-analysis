{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Road Bike Brands\n",
    "\n",
    "I am trying to figure out which road bike brand is the favorite among Redditors.\n",
    "\n",
    "For this, I am analyzing comments about bike brands from the subreddits **r/cycling**, **r/bicycling**, **r/RoadBikes** for their sentiment.\n",
    "\n",
    "### What is <span style=\"color:#FF5700\">Reddit</span>?\n",
    "\n",
    "Reddit is a website containing numerous forums about different topics. Those forums are called _Subreddits_.\n",
    "\n",
    "Each subreddit can be sorted by **new**, **hot** (ie recent and very well visited) and **top** (*of the day/the week/the month/the year/all time*).\n",
    "\n",
    "### Motivation\n",
    "\n",
    "I am hugely into cycling. Even though I dont have the money to spend on a new high-end bike, I like to dream and look at lots of different road bikes from various brands.\n",
    "\n",
    "This project stemmed from my curiosity about which brands might be the most favored among people.\n",
    "\n",
    "*Credit where credit is due*: I got the idea for that project from this [post](https://www.reddit.com/r/bicycling/comments/14ffxuy/i_analyzed_200k_comments_to_find_reddits_favorite/), but I'm putting my own twist on it.\n",
    "\n",
    "### Which brands am I looking for?\n",
    "\n",
    "- <span style=\"color:#bd82d9\">Argon 18</span>\n",
    "- <span style=\"color:#bd82d9\">Bianchi</span>, <span style=\"color:#bd82d9\">BMC</span>\n",
    "- <span style=\"color:#bd82d9\">Cannondale</span>, <span style=\"color:#bd82d9\">Canyon</span>, <span style=\"color:#bd82d9\">Cervelo</span>, <span style=\"color:#bd82d9\">Cinelli</span>, <span style=\"color:#bd82d9\">Colnago</span>, <span style=\"color:#bd82d9\">Cube</span>\n",
    "- <span style=\"color:#bd82d9\">Giant</span>\n",
    "- <span style=\"color:#bd82d9\">Merida</span>\n",
    "- <span style=\"color:#bd82d9\">Orbea</span>\n",
    "- <span style=\"color:#bd82d9\">Pinarello</span>\n",
    "- <span style=\"color:#bd82d9\">Ridley</span>, <span style=\"color:#bd82d9\">Rose</span>\n",
    "- <span style=\"color:#bd82d9\">Scott</span>, <span style=\"color:#bd82d9\">Specialized</span>\n",
    "- <span style=\"color:#bd82d9\">Trek</span>\n",
    "- <span style=\"color:#bd82d9\">Ventum</span>\n",
    "- <span style=\"color:#bd82d9\">Wilier</span>\n",
    "\n",
    "\n",
    "I originally had **Time**, **Look** and **Felt** in my list aswell, but due to those being common english words, filtering for those wasnt feasible within this timeframe.\n",
    "\n",
    "*Spoiler*: **Argon 18** and **Ventum** didn't have enough entries, so they were filtered out aswell.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "\n",
    "Since the beginning of 2024, Reddit only provides API access for a fee. I didn't want to pay that fee, so I wrote my own API:\n",
    "\n",
    "```python\n",
    "def get_posts_from_2024(endpoint, category='/hot', last_after=None, onlyId=False):\n",
    "    '''\n",
    "    This function gathers up to 1000 posts from a specified subreddit endpoint on Reddit. \n",
    "    It retrieves posts made within the year 2024 from the current runtime, excluding those posted before 2024. \n",
    "    The function can optionally return only the IDs of the posts.\n",
    "\n",
    "    :param endpoint: String representing the subreddit endpoint in the format '/r/subreddit_name'.\n",
    "    :param category: Optional string specifying the category of posts to retrieve ('/new', '/hot', or '/top', default is '/hot').\n",
    "    :param last_after: Optional string indicating the after_post_id to continue scraping from a specific point.\n",
    "    :param onlyId: Optional boolean. If True, returns a DataFrame with only the post IDs.\n",
    "    :return: A pandas DataFrame containing all post information or only the IDs if onlyId is True,\n",
    "             along with the last after_post_id for continuation of scraping.\n",
    "    '''\n",
    "\n",
    "    base_url = 'https://www.reddit.com'\n",
    "    url = base_url + endpoint + category + '.json'\n",
    "    if category == 'top/?t=year':\n",
    "        url = base_url + endpoint + '/top/' + '.json?t=year'\n",
    "    dataset = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    df = None\n",
    "\n",
    "    for i in range(10): \n",
    "        params = {\n",
    "            'limit' : 100,  # Max. amount of items per round, limited by the offical endpoint\n",
    "            't' : 'year',  # Only get posts that have been made during the last year (starting at runtime)\n",
    "            'after' : last_after  # after_post_id for next search iteration (each search is only about 25 items)\n",
    "        }\n",
    "        try:\n",
    "            response = httpx.get(url, params = params, headers=headers)\n",
    "            json_data = response.json()\n",
    "            dataset.extend([rec['data'] for rec in json_data['data']['children']])\n",
    "            last_after = json_data['data']['after']\n",
    "            print(f'Fetched {100 * i + 100} posts :)')\n",
    "            # Filtering out all posts made before 01.01.2024\n",
    "            df = pd.DataFrame(dataset)\n",
    "            start_date = datetime(2024, 1, 1, tzinfo=timezone.utc).timestamp()\n",
    "            df['created'] = df['created'].astype(float) \n",
    "            df = df[df['created'] >= start_date]\n",
    "            if onlyId:\n",
    "                df = df[['id']]\n",
    "        except: \n",
    "            print('Failed to fetch posts :(')\n",
    "            #raise\n",
    "        \n",
    "        sleeptime = float(random.randrange(2, 5))  # random sleeptime to be less suspicious\n",
    "        time.sleep(sleeptime)\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "This code returns the last 1000 posts from a given category. The 1000-post limit is set by Reddit's endpoint and can't be easily bypassed.\n",
    "\n",
    "This code snippet is followed by another one that scrapes all the top-level comments of each post using its id.\n",
    "- Using these two functions, I managed to scrape about **38,000** comments.\n",
    "\n",
    "In addition to my own scraping, I obtained a dataset of all comments from the three subreddits from 06.2005 - 12.2023 from this [torrent](https://academictorrents.com/details/56aa49f9653ba545f48df2e33679f014d2829c10).\n",
    "- This provided me with about **5,300,000** additional comments.\n",
    "\n",
    "<span style=\"color:#FF8C00\">**With that, I have a dataset that spans roughly from June 2005 to June 2024 and contains about 5,350,000 comments.**</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "After loading/scraping all data, I formatted it as | subreddit | comment |, discarding all other information from the JSON, and combined all dataframes into one large dataframe.\n",
    "\n",
    "<u>Then, I began the actual cleaning process in my module *reddit_scraper.py*:</u>\n",
    "- Converting Unicode codes into their respective characters.\n",
    "- Removing URLs.\n",
    "- Removing special characters (e.g., formatting characters).\n",
    "- Filtering out comments that do not mention any of the targeted bike brands using *TheFuzz*, which allows for typos.\n",
    "    - If a comment mentiones multiple brands, a comment is listed multiple times, each time with a different brand mentioned in that comment. \n",
    "    - *Code for that looks like this*:\n",
    "```python\n",
    "def contains_keyword(subreddit, comment, keywords):\n",
    "    '''\n",
    "    Recursively searches for all keywords in a given comment.\n",
    "    Allows for typos and is not case-sensitive.\n",
    "\n",
    "    :param subreddit: String representing the subreddit the comment is taken from.\n",
    "    :param comment: String to be parsed for keywords.\n",
    "    :param keywords: List of keywords to search for.\n",
    "    :return: List of dictionaries in JSON-like format with the shape \n",
    "             [{'subreddit': subreddit, 'keyword': keyword, 'matched_word': matched_word, 'comment': comment}]. \n",
    "             One dictionary for each keyword found.\n",
    "    '''\n",
    "    MIN_SCORE = 85\n",
    "    words = comment.lower().split()\n",
    "    best_score = 0\n",
    "    best_match = None\n",
    "    matched_word = None\n",
    "\n",
    "    for keyword in keywords:\n",
    "        for word in words:\n",
    "            score = fuzz.ratio(word, keyword.lower())\n",
    "            if score > MIN_SCORE and score > best_score:\n",
    "                best_score = score\n",
    "                best_match = keyword\n",
    "                matched_word = word\n",
    "    \n",
    "    if best_match:\n",
    "        keywords.remove(best_match)  # Remove the best match from keywords list\n",
    "\n",
    "        # Recursively call contains_keyword to find the second match\n",
    "        second_match = contains_keyword(subreddit, comment, keywords)\n",
    "\n",
    "        # Prepare data for DataFrame processing\n",
    "        match_data = [{'subreddit': subreddit, 'keyword': best_match, 'matched_word': matched_word, 'comment': comment}]\n",
    "        \n",
    "        if second_match:\n",
    "            match_data.extend(second_match)\n",
    "        \n",
    "        return match_data\n",
    "\n",
    "    return []\n",
    "```\n",
    "- Filtering out certain brands when written in lowercase, as they were often just common English words:\n",
    "    - These brands are: Giant, Cube, Rose\n",
    "- Removing brands with fewer than 100 comments.\n",
    "- Adding a column 'multiple' to mark comments that contain more than one brand and therefore appear more than once in the DataFrame.\n",
    "- The DataFrame now looks like this:\n",
    "\n",
    "| subreddit   | keyword       | matched_word   | comment                                                        | multiple |\n",
    "|-------------|---------------|----------------|----------------------------------------------------------------|----------|\n",
    "| \"bicycling\" | \"Trek\"        | \"trek\"         | \"I love Trek and Specialized. I am not a fan of Gient though!\" | true     |\n",
    "| \"bicycling\" | \"Specialized\" | \"specialized\"  | \"I love Trek and Specialized. I am not a fan of Gient though!\" | true     |\n",
    "| \"bicycling\" | \"Giant\"       | \"gient\"        | \"I love Trek and Specialized. I am not a fan of Gient though!\" | true     |\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Since certain brands are very similar to common English vocabulary, I had to filter out comments that falsely passed as being about a bike brand when they were not. I originally planned to use **NER** (Named Entity Recognition) for this, but after testing, I found that it handled most bike brands poorly.\n",
    "\n",
    "As it wasn't within my timeframe to create a new dataset and train the model on bike brands, I decided to use a more traditional approach with Keyword Based Segmentation. This method processed each comment, keeping as much context as possible while trying to remove as many other mentioned brands as possible. \n",
    "\n",
    "The DataFrame now looks like this:\n",
    "\n",
    "| subreddit   | keyword       | matched_word   | comment                          | multiple |\n",
    "|-------------|---------------|----------------|----------------------------------|----------|\n",
    "| \"bicycling\" | \"Trek\"        | \"trek\"         | \"I love Trek and Specialized.\"   | true     |\n",
    "| \"bicycling\" | \"Specialized\" | \"specialized\"  | \"I love Trek and Specialized.\"   | true     |\n",
    "| \"bicycling\" | \"Giant\"       | \"gient\"        | \"I am not a fan of Gient though! | true     |\n",
    "\n",
    "**These are the results on how many times each brand is mentioned:**\n",
    "\n",
    "| Brand       | Count  |\n",
    "|-------------|--------|\n",
    "| Trek        | 43380  |\n",
    "| Specialized | 38637  |\n",
    "| Giant       | 20464  |\n",
    "| Cannondale  | 19046  |\n",
    "| Canyon      | 11746  |\n",
    "| Bianchi     | 8563   |\n",
    "| Cervelo     | 5782   |\n",
    "| Scott       | 4639   |\n",
    "| BMC         | 2813   |\n",
    "| Pinarello   | 2784   |\n",
    "| Colnago     | 2563   |\n",
    "| Cinelli     | 2136   |\n",
    "| Orbea       | 1733   |\n",
    "| Cube        | 1589   |\n",
    "| Merida      | 1481   |\n",
    "| Ridley      | 1225   |\n",
    "| Wilier      | 830    |\n",
    "| Rose        | 761    |\n",
    "\n",
    "<u>I got an overall accuracy on 'direct-brand-hits' of 67,5%.</u>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "I performed sentiment analysis using a Hugging Face pipeline with the [roBERTa model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest). I customized some of the base settings to utilize my GPU for faster computation and adjusted the batch size to optimize performance.\n",
    "\n",
    "After conducting sentiment analysis, I filtered out comments that received a **neutral sentiment** or had a **score below 0.5** to exclude uncertain results.\n",
    "\n",
    "As a final step, I evaluated the number of comments for each brand. Brands with fewer than 100 comments were removed from further analysis due to insufficient data for meaningful insights.\n",
    "\n",
    "<br/>\n",
    "\n",
    "This is the Python code used for the sentiment analysis:\n",
    "\n",
    "```python \n",
    "def analyse_sentiment(data):\n",
    "    '''\n",
    "    Analyze sentiment of comments using a pre-trained model from Hugging Face.\n",
    "\n",
    "    :param data: DataFrame containing comments.\n",
    "    :return: DataFrame with sentiment analysis results.\n",
    "    '''\n",
    "    batch_size = 256\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        batch = data['comment'][i:i+batch_size].tolist()\n",
    "        batch_results = sentiment_task(batch)\n",
    "        results.extend(batch_results)\n",
    "\n",
    "    analytics = pd.DataFrame(data)\n",
    "    analytics['sentiment'] = [result['label'] for result in results]\n",
    "    analytics['score'] = [result['score'] for result in results]\n",
    "    \n",
    "    return analytics\n",
    "\n",
    "\n",
    "def filter_sentiment(data):\n",
    "    '''\n",
    "    Filter sentiment analysis results based on score and neutrality.\n",
    "\n",
    "    :param data: DataFrame with sentiment analysis results.\n",
    "    :return: Filtered DataFrame.\n",
    "    '''\n",
    "    filtered = data[(data['score'] >= 0.500) & (data['sentiment'] != 'neutral')]\n",
    "    filtered = remove_brand_by_threshold(filtered)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "    sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path, device=0, max_length=512, truncation=True)\n",
    "        \n",
    "    data = pd.read_json(r'data/segmented.json', orient='records', lines=True)\n",
    "    sentiment = analyse_sentiment(data)\n",
    "\n",
    "    filtered = filter_sentiment(sentiment)\n",
    "    filtered.to_json(r'data/sentiment_filtered.json', orient='records', lines=True)\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcasing Results\n",
    "\n",
    "**I visualized the main result as a bar chart using the respective logos of each brand, similar to how it was done in my inspiration.**\n",
    "\n",
    "In addition to that, I have visualized the following three results:\n",
    "\n",
    "- **Top 3 Liked Brands of Each Subreddit**\n",
    "- **Top 3 Most 'Hated' Brands of Each Subreddit**\n",
    "- **Most Controversial Brands Based on Controversy Score**\n",
    "\n",
    "$ \\text{Controversy Score} = \\frac{\\text{Bad Comments}}{\\text{Good Comments} + \\text{Bad Comments}} \\times \\left(1 - \\frac{\\text{Total Comments}}{\\text{Maximum Comments}}\\right) $\n",
    "\n",
    "<small>\n",
    "\n",
    "- <b>Bad Comments</b>: Number of negative or unfavorable comments about the brand.\n",
    "- <b>Good Comments</b>: Number of positive or favorable comments about the brand.\n",
    "- <b>Total Comments</b>: Total number of comments (both good and bad) about the brand.\n",
    "- <b>Maximum Comments</b>: A hypothetical maximum number of comments used to scale the influence of total comments on the controversy score.\n",
    "</small>\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./plots/vis1.png\" alt=\"Image 4\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./plots/vis2.png\" alt=\"Image 3\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./plots/vis31.png\" alt=\"Image 1\" style=\"max-width: 45%; margin: 5px;\">\n",
    "  <img src=\"./plots/vis32.png\" alt=\"Image 2\" style=\"max-width: 45%; margin: 5px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "```\n",
    " o__         __o        ,__o        __o           __o\n",
    " ,>/_       -\\<,      _-\\_<,       _`\\<,_       _ \\<_\n",
    "(*)`(*).....O/ O.....(*)/'(*).....(*)/ (*).....(_)/(_)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
